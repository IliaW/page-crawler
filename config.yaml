env: "local" # Use 'local' for LocalStack
log_level: "debug"
log_type: "text" # 'text' or 'json'. Text type has colorized error levels
service_name: "page-crawler-worker"
port: "8080"
version: "1.0.0"

worker:
  workers_num: 1000 # Number of workers. Set -1 to use runtime.NumCPU()
  crawl_mechanism: 0 # 0 -'curl', 1 - 'headless browser'(beta)
  retry_attempts: 2 # The number of attempts to crawl the page if 429 status code is received.
  retry_delay: "3s" # The delay between retries. The number will increase exponentially.
  user_agent: "web-crawler-bot"

cache:
  servers: "cache:11211"
  ttl_for_page: "168h"

database:
  host: "db"
  port: "5432"
  user: "web_crawler_rw_user"
  password: "test"
  name: "web_crawler_rds_psql"
  conn_max_lifetime: "10m"
  max_open_conns: 10
  max_idle_conns: 5

s3:
  aws_base_endpoint: "" # Used when 'local' env. (For LocalStack)
  region: "us-east-1"
  bucket_name: "web-crawler-v1-local"
  key_prefix: "page-crawler"

kafka:
  producer:
    addr: "kafka:9092"
    write_topic_name: "page-crawler-content-processor-topic-v1-local"
    dlq_topic_name: "page-crawler-dlq-topic-v1-local"
    max_attempts: 3 # Number of attempts to send a message to Kafka
    batch_size: 1000 # Number of messages to batch before sending to Kafka
    batch_timeout: "2s" # The time after which messages will be sent to Kafka, even if the batch_size has not been reached (has custom implementation)
    read_timeout: "10s"
    write_timeout: "10s"
    required_acks: 1 # Number of acknowledges: 0 - fire-and-forget, 1 - wait for the leader, -1 - wait for all
    async: false # If true - no guarantees of whether the messages were written to Kafka
  consumer:
    brokers: "kafka:9092"
    read_topic_name: "web-crawler-page-crawler-topic-v1-local"
    group_id: "page-crawler-kafka-group-v1-local"
    max_wait: "500ms" # Maximum amount of time to wait for new data to come
    read_batch_timeout: "2s"
    queue_capacity: 1000 # The maximum number of messages that can be buffered in the consumer queue
    max_bytes: 10e6
    commit_interval: "1s"

crawler: # common-crawl starts sending 5xx errors when exceeding some thresholds
  request_timeout: 15 # in seconds
  retries: 1
  last_crawl_indexes: 1 # number of the newest indexes where the URL search will be performed.

telemetry:
  enabled: true
  collector_url: "localhost:4318"

http_client:
  request_timeout: "15s"
  max_idle_connections: 800
  max_idle_connections_per_host: 10
  max_connections_per_host: 10 # good to have it the same as a cache.threshold in url-gate
  idle_connection_timeout: "15s"
  tls_handshake_timeout: "10s"
  dial_timeout: "10s"
  dial_keep_alive: "20s"
  tls_insecure_skip_verify: true # If true - the client will not verify the server's certificate
